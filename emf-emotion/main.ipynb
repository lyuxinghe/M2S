{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyedflib\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import FastICA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import Subject_Session_Data\n",
    "#from preprocess import butter_bandpass_filter\n",
    "#from feature_extraction import EEGFeatureExtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "        work_dir = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "        dataset_dir = os.path.join(work_dir, \"enterface06_EMOBRAIN\", \"Data\")\n",
    "        dataset_common_dir = os.path.join(dataset_dir, \"Common\")\n",
    "        dataset_EEG_dir = os.path.join(dataset_dir, \"EEG\")\n",
    "        dataset_fNIRS_dir = os.path.join(dataset_dir, \"fNIRS\")\n",
    "\n",
    "        print(\"working with dataset directory:\", dataset_dir)\n",
    "\n",
    "        # Construct the search pattern\n",
    "        pattern = os.path.join(dataset_EEG_dir, \"*.bdf\")\n",
    "\n",
    "        # Find all files in the directory matching the pattern\n",
    "        bdf_files = glob.glob(pattern)\n",
    "\n",
    "        count = 1\n",
    "        data_list = []\n",
    "        for file in bdf_files:\n",
    "                # Open the BDF file\n",
    "                try:\n",
    "                        with pyedflib.EdfReader(file) as f:\n",
    "                                # marks = []\n",
    "                                print(\"reading file :\", count)\n",
    "                                participant_index = file.find(\"Part\")\n",
    "                                session_index = file.find(\"SES\")\n",
    "                                participant_number = int(file[participant_index+4])\n",
    "                                # print(\"Participant number:\", participant_number)\n",
    "                                session_number = int(file[session_index+3])\n",
    "                                # print(\"Session number:\", session_number)\n",
    "                                if participant_number == 2 and session_number == 1:\n",
    "                                        continue\n",
    "                                fi = open(file+\".mrk\", \"r\")\n",
    "                                fi.readline()\n",
    "                                inferred_temp_marks = []\n",
    "                                for line in fi:\n",
    "                                        temp_marks = line.split('\\t')[1:]\n",
    "                                        temp_marks[-1] = temp_marks[-1][:-1]\n",
    "                                        if temp_marks[-1] == '\"255\"':\n",
    "                                                if temp_marks[0] == temp_marks[1]:\n",
    "                                                        inferred_temp_marks.append(int(temp_marks[0]))\n",
    "                                                else:\n",
    "                                                        print(\"Irregular:\", temp_marks)\n",
    "\n",
    "                                block_sample_ranges = []\n",
    "                                for trigger in inferred_temp_marks:\n",
    "                                        if participant_number == 1 and session_number == 1:\n",
    "                                                start_index = trigger + 768\n",
    "                                                end_index = start_index + 3200\n",
    "                                                block_sample_ranges.append((start_index,end_index))\n",
    "                                        else:\n",
    "                                                start_index = trigger + 3072\n",
    "                                                end_index = start_index + 12800\n",
    "                                                block_sample_ranges.append((start_index,end_index))\n",
    "\n",
    "                                data_list.append(Subject_Session_Data(f, count, participant_number, session_number, inferred_temp_marks, block_sample_ranges))\n",
    "                                '''\n",
    "                                # Get general information\n",
    "                                print(\"reading file :\", count)\n",
    "                                print(\"File duration in seconds:\", f.file_duration)\n",
    "                                print(\"Number of signals:\", f.signals_in_file)\n",
    "                                signal_labels = f.getSignalLabels()\n",
    "                                print(\"Signal labels:\", signal_labels)\n",
    "\n",
    "                                # Read data from each signal\n",
    "                                for i in range(f.signals_in_file):\n",
    "                                        data = f.readSignal(i)\n",
    "                                        print(f\"Data from signal {signal_labels[i]}:\", data)\n",
    "                                '''\n",
    "                except (OSError):\n",
    "                        print(\"reading file :\", count, \"FAIL\")\n",
    "                count += 1\n",
    "\n",
    "        emotion_classes = os.path.join(dataset_common_dir, \"IAPS_Classes_EEG_fNIRS.txt\")\n",
    "        emotion_file = open(emotion_classes, \"r\")\n",
    "        session1 = []\n",
    "        session2 = []\n",
    "        session3 = []\n",
    "        for line in emotion_file:\n",
    "                temp_emotions = line.split(\"\\t\")\n",
    "                temp_emotions[-1] = temp_emotions[-1][:-1]\n",
    "                session1.append(temp_emotions[0])\n",
    "                session2.append(temp_emotions[1])\n",
    "                session3.append(temp_emotions[2])\n",
    "\n",
    "        for subject_session in data_list:\n",
    "                if subject_session.session_number == 1:\n",
    "                        subject_session.emotions = session1\n",
    "                elif subject_session.session_number == 2:\n",
    "                        subject_session.emotions = session2\n",
    "                elif subject_session.session_number == 3:\n",
    "                        subject_session.emotions = session3\n",
    "\n",
    "        ################################ VALIDATION OF .MRK FILE DO NOT DELETE ##############################\n",
    "        # seconds = {}\n",
    "        # j_index = 0\n",
    "        # for j in range(len(data_list)):\n",
    "        #         flag = 0\n",
    "        #         print(\"Number of marks in session:\", len(data_list[j].marks))\n",
    "        #         print(\"Number of emotions:\", len(data_list[j].emotions))\n",
    "        #         print(data_list[j].marks)\n",
    "        #         print(\"number of signals:\", len(data_list[j].data_dictionary[data_list[j].signal_labels[0]]))\n",
    "        #         print(\"participant number:\", data_list[j].participant_number)\n",
    "        #         print(\"session number:\", data_list[j].session_number)\n",
    "                \n",
    "        #         for i in range(len(data_list[j].marks)):\n",
    "        #                 if i == 0:\n",
    "        #                         continue\n",
    "        #                 else:\n",
    "        #                         if data_list[j].participant_number == 1 and data_list[j].session_number == 1:\n",
    "        #                                 temp_seconds = (data_list[j].marks[i] - data_list[j].marks[i-1])/256\n",
    "        #                                 flag = 1\n",
    "        #                         else:\n",
    "        #                                 temp_seconds = (data_list[j].marks[i] - data_list[j].marks[i-1])/1024\n",
    "        #                         if flag == 1:\n",
    "        #                                 j_index = j\n",
    "\n",
    "        #                         if j in seconds.keys():\n",
    "        #                                 seconds[j].append(temp_seconds)\n",
    "        #                         else:\n",
    "        #                                 seconds[j] = [temp_seconds]\n",
    "\n",
    "        # for key in seconds.keys():\n",
    "        #         if key == j_index:\n",
    "        #                 print(\"First participant, first session\")\n",
    "        #                 print(\"Minimum time for one trial:\", min(seconds[key]))\n",
    "        #                 print(\"Maximum time for one trial:\", max(seconds[key]))\n",
    "        #                 print(\"Mean time for one trial:\", sum(seconds[key])/len(seconds[key]))\n",
    "        #         else:\n",
    "        #                 print(\"Minimum time for one trial:\", min(seconds[key]))\n",
    "        #                 print(\"Maximum time for one trial:\", max(seconds[key]))\n",
    "        #                 print(\"Mean time for one trial:\", sum(seconds[key])/len(seconds[key]))\n",
    "\n",
    "        return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working with dataset directory: c:\\Users\\LyuxingHe\\Desktop\\GitRepos\\M2S\\enterface06_EMOBRAIN\\Data\n",
      "reading file : 1\n",
      "reading file : 2\n",
      "reading file : 3\n",
      "reading file : 4\n",
      "reading file : 4\n",
      "reading file : 5\n",
      "reading file : 6\n",
      "reading file : 7\n",
      "reading file : 8\n",
      "reading file : 9\n",
      "reading file : 10\n",
      "reading file : 11\n",
      "reading file : 12\n",
      "reading file : 13\n",
      "reading file : 14\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file number: 3\n",
      "File duration in seconds: 1158\n",
      "Number of channels: 72\n",
      "Number of samples: 1185792\n",
      "Participant number: 1\n",
      "Session number: 3\n",
      "Emotions: ['Pos', 'Neg', 'Calm', 'Pos', 'Pos', 'Neg', 'Calm', 'Neg', 'Calm', 'Calm', 'Neg', 'Neg', 'Calm', 'Pos', 'Pos', 'Calm', 'Neg', 'Pos', 'Neg', 'Calm', 'Pos', 'Calm', 'Calm', 'Neg', 'Pos', 'Calm', 'Pos', 'Pos', 'Neg', 'Neg']\n",
      "Signal labels: ['Fp1', 'AF7', 'AF3', 'F1', 'F3', 'F5', 'F7', 'FT7', 'FC5', 'FC3', 'FC1', 'C1', 'C3', 'C5', 'T7', 'TP7', 'CP5', 'CP3', 'CP1', 'P1', 'P3', 'P5', 'P7', 'P9', 'PO7', 'PO3', 'O1', 'Iz', 'Oz', 'POz', 'Pz', 'CPz', 'Fpz', 'Fp2', 'AF8', 'AF4', 'AFz', 'Fz', 'F2', 'F4', 'F6', 'F8', 'FT8', 'FC6', 'FC4', 'FC2', 'FCz', 'Cz', 'C2', 'C4', 'C6', 'T8', 'TP8', 'CP6', 'CP4', 'CP2', 'P2', 'P4', 'P6', 'P8', 'P10', 'PO8', 'PO4', 'O2', 'GSR1', 'GSR2', 'Erg1', 'Erg2', 'Resp', 'Plet', 'Temp', 'Status']\n"
     ]
    }
   ],
   "source": [
    "dataset[2].display_info()\n",
    "sample_data = dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(15970, 28770), (87669, 100469), (123319, 136119), (161059, 173859), (196729, 209529), (235734, 248534), (269850, 282650), (307086, 319886), (354439, 367239), (393163, 405963), (430615, 443415), (466212, 479012), (503643, 516443), (540469, 553269), (578306, 591106), (614546, 627346), (651215, 664015), (690152, 702952), (724799, 737599), (761431, 774231), (799082, 811882), (835172, 847972), (870946, 883746), (910581, 923381), (944273, 957073), (980509, 993309), (1014399, 1027199), (1049390, 1062190), (1082367, 1095167), (1118888, 1131688)]\n"
     ]
    }
   ],
   "source": [
    "# shows the 12.5s truncation index\n",
    "print(sample_data.block_sample_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy import signal\n",
    "\n",
    "from scipy.signal import butter, sosfiltfilt\n",
    "\n",
    "# Bandpass filter\n",
    "# For: Environmental noise removal by applying a bandpass filter in the 4-45 Hz range\n",
    "# fs: sample rate, lowcut/ highcut: band cutoffs\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=3):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    #sos = butter(order, [low, high], analog=False, fs=fs, btype='band', output='sos')\n",
    "    sos = butter(order, [lowcut, highcut], analog=False, fs=fs, btype='band', output='sos')\n",
    "    filtered = sosfiltfilt(sos, data)\n",
    "    return filtered\n",
    "\n",
    "class EEGFeatureExtraction:\n",
    "    \n",
    "    def __init__(self, data, fs=1024):\n",
    "        self.data = data\n",
    "        self.fs = fs\n",
    "        self.num_samples = len(data)\n",
    "        self.num_channels = data[1].num_channels\n",
    "        self.num_truncations = len(data[1].block_sample_ranges)\n",
    "        self.target_channels = ['F3', 'F4', 'T7', 'Fp1', 'Fp2', 'T8', 'F7', 'F8', 'O1', 'P7', 'P8', 'O2']\n",
    "        print()\n",
    "\n",
    "    def display_info(self):\n",
    "            # samples = {}\n",
    "            print(\"Number of samples:\", self.num_samples)\n",
    "            print(\"Number of channels:\", self.num_channels)\n",
    "            print(\"Number of truncations:\", self.num_truncations)\n",
    "            print(\"Target channels:\", self.target_channels)\n",
    "\n",
    "    def extract_features(self):\n",
    "        results = np.zeros(shape = (self.num_samples, len(self.target_channels) * self.num_truncations))\n",
    "        for sample_id, sample in enumerate(self.data):\n",
    "            for channel_id, channel in enumerate(self.target_channels):\n",
    "                assert channel in sample.signal_labels, channel + \" target channel is not present in the sample data!\"\n",
    "                channel_signal = sample.data_dictionary[channel]\n",
    "                for band_id, truncation_range in enumerate(sample.block_sample_ranges):\n",
    "                    truncation = channel_signal[truncation_range[0]:truncation_range[1]]\n",
    "                    filtered = butter_bandpass_filter(truncation, 4, 45, self.fs)\n",
    "                    f, Pxx_den = signal.welch(truncation, self.fs, nperseg=256, noverlap=128)\n",
    "                    #print Pxx_den.shape, Pxx_den\n",
    "                \n",
    "                    results[sample_id][channel_id*self.num_truncations + band_id] = math.log(np.max(Pxx_den))\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of samples: 14\n",
      "Number of channels: 72\n",
      "Number of truncations: 30\n",
      "Target channels: ['F3', 'F4', 'T7', 'Fp1', 'Fp2', 'T8', 'F7', 'F8', 'O1', 'P7', 'P8', 'O2']\n"
     ]
    }
   ],
   "source": [
    "eeg_fe = EEGFeatureExtraction(dataset)\n",
    "eeg_fe.display_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_features = eeg_fe.extract_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_labels = np.array([sample.emotions for sample in dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature shape: (14, 360)\n",
      "label shape: (14, 30)\n"
     ]
    }
   ],
   "source": [
    "print(\"feature shape:\", eeg_features.shape)\n",
    "print(\"label shape:\", eeg_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature shape: torch.Size([420, 12])\n",
      "label shape: torch.Size([420])\n",
      "Unique encoded labels: [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "reshaped_features = eeg_features.reshape(14*30, -1)  # (420, 12)\n",
    "flattened_labels = eeg_labels.flatten()\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(flattened_labels)\n",
    "integer_encoded = integer_encoded.reshape(-1, 1)\n",
    "\n",
    "features_tensor = torch.tensor(reshaped_features, dtype=torch.float32)\n",
    "labels_tensor = torch.tensor(integer_encoded, dtype=torch.long).squeeze()\n",
    "print(\"feature shape:\", features_tensor.shape)\n",
    "print(\"label shape:\", labels_tensor.shape)\n",
    "print(\"Unique encoded labels:\", np.unique(integer_encoded))\n",
    "\n",
    "tensor_dataset = TensorDataset(features_tensor, labels_tensor)\n",
    "\n",
    "train_dataset, test_dataset = train_test_split(tensor_dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(12, 32)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.fc3 = nn.Linear(64, 128)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.bn4 = nn.BatchNorm1d(64)\n",
    "        self.fc5 = nn.Linear(64, 32)\n",
    "        self.bn5 = nn.BatchNorm1d(32)\n",
    "        self.fc6 = nn.Linear(32, 3)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.bn4(self.fc4(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.bn5(self.fc5(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.1648165688795202\n",
      "Epoch 2, Loss: 1.1336227339856766\n",
      "Epoch 3, Loss: 1.1324356654111076\n",
      "Epoch 4, Loss: 1.0894155379603891\n",
      "Epoch 5, Loss: 1.0934633928186752\n",
      "Epoch 6, Loss: 1.0954632145517014\n",
      "Epoch 7, Loss: 1.0881660843596739\n",
      "Epoch 8, Loss: 1.1042397793601542\n",
      "Epoch 9, Loss: 1.0812998221201056\n",
      "Epoch 10, Loss: 1.0800842502537895\n",
      "Epoch 11, Loss: 1.1137573350878323\n",
      "Epoch 12, Loss: 1.0941436062840855\n",
      "Epoch 13, Loss: 1.0937685160075916\n",
      "Epoch 14, Loss: 1.0820258014342363\n",
      "Epoch 15, Loss: 1.0929734829594107\n",
      "Epoch 16, Loss: 1.087516542743234\n",
      "Epoch 17, Loss: 1.091126848669613\n",
      "Epoch 18, Loss: 1.085833696758046\n",
      "Epoch 19, Loss: 1.0669370644232805\n",
      "Epoch 20, Loss: 1.0929778060492348\n",
      "Epoch 21, Loss: 1.0986194435287924\n",
      "Epoch 22, Loss: 1.111081121598973\n",
      "Epoch 23, Loss: 1.1011456952375525\n",
      "Epoch 24, Loss: 1.0849248416283552\n",
      "Epoch 25, Loss: 1.0771573694313274\n",
      "Epoch 26, Loss: 1.0863752382643082\n",
      "Epoch 27, Loss: 1.0715318813043482\n",
      "Epoch 28, Loss: 1.0777399837970734\n",
      "Epoch 29, Loss: 1.084567368030548\n",
      "Epoch 30, Loss: 1.086298274643281\n",
      "Epoch 31, Loss: 1.0863641237511354\n",
      "Epoch 32, Loss: 1.0993570054278654\n",
      "Epoch 33, Loss: 1.0725425920065712\n",
      "Epoch 34, Loss: 1.0736620110623978\n",
      "Epoch 35, Loss: 1.0743170450715458\n",
      "Epoch 36, Loss: 1.0796907456482159\n",
      "Epoch 37, Loss: 1.083215529427809\n",
      "Epoch 38, Loss: 1.0820314007646896\n",
      "Epoch 39, Loss: 1.0835234820842743\n",
      "Epoch 40, Loss: 1.0893800258636475\n",
      "Epoch 41, Loss: 1.08131423943183\n",
      "Epoch 42, Loss: 1.0694480485775892\n",
      "Epoch 43, Loss: 1.0813931963023018\n",
      "Epoch 44, Loss: 1.087170553558013\n",
      "Epoch 45, Loss: 1.0741422491915085\n",
      "Epoch 46, Loss: 1.078617679722169\n",
      "Epoch 47, Loss: 1.048451222041074\n",
      "Epoch 48, Loss: 1.0783966972547419\n",
      "Epoch 49, Loss: 1.067704539088642\n",
      "Epoch 50, Loss: 1.0853400545961716\n",
      "Epoch 51, Loss: 1.0732945694642908\n",
      "Epoch 52, Loss: 1.067051698179806\n",
      "Epoch 53, Loss: 1.0817083243061514\n",
      "Epoch 54, Loss: 1.070674933054868\n",
      "Epoch 55, Loss: 1.0882950635517346\n",
      "Epoch 56, Loss: 1.073539286851883\n",
      "Epoch 57, Loss: 1.0606081012417288\n",
      "Epoch 58, Loss: 1.0730706365669476\n",
      "Epoch 59, Loss: 1.0869574038421406\n",
      "Epoch 60, Loss: 1.0714098898803486\n",
      "Epoch 61, Loss: 1.072893854449777\n",
      "Epoch 62, Loss: 1.0688657357412226\n",
      "Epoch 63, Loss: 1.0623226498856264\n",
      "Epoch 64, Loss: 1.0726867195437937\n",
      "Epoch 65, Loss: 1.0633555510464836\n",
      "Epoch 66, Loss: 1.0552485006697037\n",
      "Epoch 67, Loss: 1.0579845081357395\n",
      "Epoch 68, Loss: 1.0664126382154577\n",
      "Epoch 69, Loss: 1.0855652391910553\n",
      "Epoch 70, Loss: 1.0782418426345377\n",
      "Epoch 71, Loss: 1.0774023988667656\n",
      "Epoch 72, Loss: 1.0685845034963943\n",
      "Epoch 73, Loss: 1.0694714328821968\n",
      "Epoch 74, Loss: 1.0666358646224527\n",
      "Epoch 75, Loss: 1.084054122952854\n",
      "Epoch 76, Loss: 1.0729818326585434\n",
      "Epoch 77, Loss: 1.0742958836695726\n",
      "Epoch 78, Loss: 1.0676618271014269\n",
      "Epoch 79, Loss: 1.082438686314751\n",
      "Epoch 80, Loss: 1.0730334555401522\n",
      "Epoch 81, Loss: 1.078593802802703\n",
      "Epoch 82, Loss: 1.0937000583199894\n",
      "Epoch 83, Loss: 1.084200317368788\n",
      "Epoch 84, Loss: 1.064028082525029\n",
      "Epoch 85, Loss: 1.0767489023068373\n",
      "Epoch 86, Loss: 1.080489372505861\n",
      "Epoch 87, Loss: 1.068029554451213\n",
      "Epoch 88, Loss: 1.0778370569734013\n",
      "Epoch 89, Loss: 1.0835355415063745\n",
      "Epoch 90, Loss: 1.0785929245107315\n",
      "Epoch 91, Loss: 1.0607884946991415\n",
      "Epoch 92, Loss: 1.0811934541253483\n",
      "Epoch 93, Loss: 1.079434128368602\n",
      "Epoch 94, Loss: 1.0540139044032377\n",
      "Epoch 95, Loss: 1.0626927018165588\n",
      "Epoch 96, Loss: 1.0705284598995657\n",
      "Epoch 97, Loss: 1.058365853393779\n",
      "Epoch 98, Loss: 1.0520399808883667\n",
      "Epoch 99, Loss: 1.0562941431999207\n",
      "Epoch 100, Loss: 1.081796465551152\n",
      "Epoch 101, Loss: 1.0698893245528727\n",
      "Epoch 102, Loss: 1.0691944693817812\n",
      "Epoch 103, Loss: 1.051258805920096\n",
      "Epoch 104, Loss: 1.074503416524214\n",
      "Epoch 105, Loss: 1.054403906359392\n",
      "Epoch 106, Loss: 1.0887894262285793\n",
      "Epoch 107, Loss: 1.0667414349668167\n",
      "Epoch 108, Loss: 1.0720093372990103\n",
      "Epoch 109, Loss: 1.058768111116746\n",
      "Epoch 110, Loss: 1.0594766350353466\n",
      "Epoch 111, Loss: 1.0681459868655485\n",
      "Epoch 112, Loss: 1.0353190969018375\n",
      "Epoch 113, Loss: 1.045431813772987\n",
      "Epoch 114, Loss: 1.0945025717510897\n",
      "Epoch 115, Loss: 1.0699598754153532\n",
      "Epoch 116, Loss: 1.0699663565439337\n",
      "Epoch 117, Loss: 1.0682348170701195\n",
      "Epoch 118, Loss: 1.087774066364064\n",
      "Epoch 119, Loss: 1.07120578604586\n",
      "Epoch 120, Loss: 1.0673452265122358\n",
      "Epoch 121, Loss: 1.0529517180779402\n",
      "Epoch 122, Loss: 1.0658986060058369\n",
      "Epoch 123, Loss: 1.0733654148438398\n",
      "Epoch 124, Loss: 1.0691600038724787\n",
      "Epoch 125, Loss: 1.0737184075748218\n",
      "Epoch 126, Loss: 1.0477595101384556\n",
      "Epoch 127, Loss: 1.0855971189106213\n",
      "Epoch 128, Loss: 1.061851227984709\n",
      "Epoch 129, Loss: 1.0680969971067764\n",
      "Epoch 130, Loss: 1.078436909353032\n",
      "Epoch 131, Loss: 1.07369261804749\n",
      "Epoch 132, Loss: 1.0738228541963242\n",
      "Epoch 133, Loss: 1.0711524924811195\n",
      "Epoch 134, Loss: 1.0682730026104872\n",
      "Epoch 135, Loss: 1.0667648893945358\n",
      "Epoch 136, Loss: 1.0620198670555563\n",
      "Epoch 137, Loss: 1.0753885349806618\n",
      "Epoch 138, Loss: 1.060990975183599\n",
      "Epoch 139, Loss: 1.0605749943677116\n",
      "Epoch 140, Loss: 1.071455503211302\n",
      "Epoch 141, Loss: 1.069902299081578\n",
      "Epoch 142, Loss: 1.0630187357173246\n",
      "Epoch 143, Loss: 1.0598899687037748\n",
      "Epoch 144, Loss: 1.054669488878811\n",
      "Epoch 145, Loss: 1.0674024013911976\n",
      "Epoch 146, Loss: 1.072103582760867\n",
      "Epoch 147, Loss: 1.050339111510445\n",
      "Epoch 148, Loss: 1.069589178351795\n",
      "Epoch 149, Loss: 1.0552949677495396\n",
      "Epoch 150, Loss: 1.095968905617209\n",
      "Epoch 151, Loss: 1.0518905800931595\n",
      "Epoch 152, Loss: 1.0469894233871908\n",
      "Epoch 153, Loss: 1.037478555651272\n",
      "Epoch 154, Loss: 1.0741908778162563\n",
      "Epoch 155, Loss: 1.0783275961875916\n",
      "Epoch 156, Loss: 1.0814917297924267\n",
      "Epoch 157, Loss: 1.0703280024668749\n",
      "Epoch 158, Loss: 1.063190041219487\n",
      "Epoch 159, Loss: 1.0689702489796806\n",
      "Epoch 160, Loss: 1.0602739102700178\n",
      "Epoch 161, Loss: 1.0439938955447252\n",
      "Epoch 162, Loss: 1.0717239046798033\n",
      "Epoch 163, Loss: 1.07209610062487\n",
      "Epoch 164, Loss: 1.0725534540765427\n",
      "Epoch 165, Loss: 1.0693413352265078\n",
      "Epoch 166, Loss: 1.0642114737454582\n",
      "Epoch 167, Loss: 1.0634378808386185\n",
      "Epoch 168, Loss: 1.0704564522294437\n",
      "Epoch 169, Loss: 1.0616749963339638\n",
      "Epoch 170, Loss: 1.0676410548827227\n",
      "Epoch 171, Loss: 1.0593150065225714\n",
      "Epoch 172, Loss: 1.056857894448673\n",
      "Epoch 173, Loss: 1.0614779644152696\n",
      "Epoch 174, Loss: 1.0765225326313692\n",
      "Epoch 175, Loss: 1.0771726299734676\n",
      "Epoch 176, Loss: 1.0533674324260038\n",
      "Epoch 177, Loss: 1.0664385750013239\n",
      "Epoch 178, Loss: 1.067935750764959\n",
      "Epoch 179, Loss: 1.049410812995013\n",
      "Epoch 180, Loss: 1.0486843743745018\n",
      "Epoch 181, Loss: 1.0430879207218395\n",
      "Epoch 182, Loss: 1.0567748266107895\n",
      "Epoch 183, Loss: 1.0651684964404387\n",
      "Epoch 184, Loss: 1.0381067132248598\n",
      "Epoch 185, Loss: 1.0863635101739098\n",
      "Epoch 186, Loss: 1.092770806130241\n",
      "Epoch 187, Loss: 1.053388115237741\n",
      "Epoch 188, Loss: 1.0601080095066744\n",
      "Epoch 189, Loss: 1.073446843553992\n",
      "Epoch 190, Loss: 1.0601941645145416\n",
      "Epoch 191, Loss: 1.0698013708871954\n",
      "Epoch 192, Loss: 1.0588739900028004\n",
      "Epoch 193, Loss: 1.058608805432039\n",
      "Epoch 194, Loss: 1.0798134593402637\n",
      "Epoch 195, Loss: 1.0737274850116056\n",
      "Epoch 196, Loss: 1.0391198256436516\n",
      "Epoch 197, Loss: 1.0521645931636585\n",
      "Epoch 198, Loss: 1.0665478355744307\n",
      "Epoch 199, Loss: 1.0611047955120312\n",
      "Epoch 200, Loss: 1.06845141509\n",
      "Epoch 201, Loss: 1.0655490124926847\n",
      "Epoch 202, Loss: 1.0502322210985071\n",
      "Epoch 203, Loss: 1.049668078913408\n",
      "Epoch 204, Loss: 1.05008468207191\n",
      "Epoch 205, Loss: 1.0659308433532715\n",
      "Epoch 206, Loss: 1.0661807112834032\n",
      "Epoch 207, Loss: 1.0672036952832167\n",
      "Epoch 208, Loss: 1.0471616019220913\n",
      "Epoch 209, Loss: 1.0604504679932314\n",
      "Epoch 210, Loss: 1.0597815548672396\n",
      "Epoch 211, Loss: 1.0644713044166565\n",
      "Epoch 212, Loss: 1.0699722731814665\n",
      "Epoch 213, Loss: 1.0522057308870203\n",
      "Epoch 214, Loss: 1.0468696723966038\n",
      "Epoch 215, Loss: 1.0849886238574982\n",
      "Epoch 216, Loss: 1.0549640620456022\n",
      "Epoch 217, Loss: 1.0814869298654444\n",
      "Epoch 218, Loss: 1.083941522766562\n",
      "Epoch 219, Loss: 1.0706921026987188\n",
      "Epoch 220, Loss: 1.0551579086219562\n",
      "Epoch 221, Loss: 1.0620601124623243\n",
      "Epoch 222, Loss: 1.07855485292042\n",
      "Epoch 223, Loss: 1.0592294230180628\n",
      "Epoch 224, Loss: 1.0578120946884155\n",
      "Epoch 225, Loss: 1.0719825117027058\n",
      "Epoch 226, Loss: 1.0647088885307312\n",
      "Epoch 227, Loss: 1.0414040912600124\n",
      "Epoch 228, Loss: 1.0420346645747913\n",
      "Epoch 229, Loss: 1.0797277916880215\n",
      "Epoch 230, Loss: 1.0492038533968084\n",
      "Epoch 231, Loss: 1.0715086284805746\n",
      "Epoch 232, Loss: 1.0697871990063612\n",
      "Epoch 233, Loss: 1.0522343761780684\n",
      "Epoch 234, Loss: 1.0707949294763452\n",
      "Epoch 235, Loss: 1.061293140930288\n",
      "Epoch 236, Loss: 1.074445012737723\n",
      "Epoch 237, Loss: 1.0882519080358393\n",
      "Epoch 238, Loss: 1.069309585234698\n",
      "Epoch 239, Loss: 1.080413849914775\n",
      "Epoch 240, Loss: 1.0644779100137598\n",
      "Epoch 241, Loss: 1.0720524121733273\n",
      "Epoch 242, Loss: 1.057177005445256\n",
      "Epoch 243, Loss: 1.0568008405320786\n",
      "Epoch 244, Loss: 1.0694701583946453\n",
      "Epoch 245, Loss: 1.051984916715061\n",
      "Epoch 246, Loss: 1.0565798387807959\n",
      "Epoch 247, Loss: 1.0634305652450113\n",
      "Epoch 248, Loss: 1.0773323721745436\n",
      "Epoch 249, Loss: 1.0370479506604813\n",
      "Epoch 250, Loss: 1.0464873401557697\n",
      "Epoch 251, Loss: 1.073322562610402\n",
      "Epoch 252, Loss: 1.085639567936168\n",
      "Epoch 253, Loss: 1.0747884003555073\n",
      "Epoch 254, Loss: 1.060736086438684\n",
      "Epoch 255, Loss: 1.052031629225787\n",
      "Epoch 256, Loss: 1.0530014791909386\n",
      "Epoch 257, Loss: 1.0711982425521402\n",
      "Epoch 258, Loss: 1.052573842160842\n",
      "Epoch 259, Loss: 1.064213349538691\n",
      "Epoch 260, Loss: 1.062340194688124\n",
      "Epoch 261, Loss: 1.055547163766973\n",
      "Epoch 262, Loss: 1.0448144849608927\n",
      "Epoch 263, Loss: 1.0633369175826801\n",
      "Epoch 264, Loss: 1.0440892559640549\n",
      "Epoch 265, Loss: 1.0498323878821205\n",
      "Epoch 266, Loss: 1.041460543870926\n",
      "Epoch 267, Loss: 1.0714967829339646\n",
      "Epoch 268, Loss: 1.0506837876404034\n",
      "Epoch 269, Loss: 1.0600155066041386\n",
      "Epoch 270, Loss: 1.0761091884444742\n",
      "Epoch 271, Loss: 1.0607274051974802\n",
      "Epoch 272, Loss: 1.0562277716748856\n",
      "Epoch 273, Loss: 1.07536983665298\n",
      "Epoch 274, Loss: 1.0531804999884438\n",
      "Epoch 275, Loss: 1.0548699392991907\n",
      "Epoch 276, Loss: 1.067079551079694\n",
      "Epoch 277, Loss: 1.0603579089922064\n",
      "Epoch 278, Loss: 1.0438188822830425\n",
      "Epoch 279, Loss: 1.0411379354841568\n",
      "Epoch 280, Loss: 1.0368695679832907\n",
      "Epoch 281, Loss: 1.043984609491685\n",
      "Epoch 282, Loss: 1.0483122450463913\n",
      "Epoch 283, Loss: 1.0652733567882986\n",
      "Epoch 284, Loss: 1.0508746522314407\n",
      "Epoch 285, Loss: 1.0694303863188799\n",
      "Epoch 286, Loss: 1.0657886897816378\n",
      "Epoch 287, Loss: 1.0445257092223448\n",
      "Epoch 288, Loss: 1.0529051440603592\n",
      "Epoch 289, Loss: 1.0573834198362686\n",
      "Epoch 290, Loss: 1.0321915167219498\n",
      "Epoch 291, Loss: 1.0439496145528906\n",
      "Epoch 292, Loss: 1.0710734041298138\n",
      "Epoch 293, Loss: 1.0520095246679642\n",
      "Epoch 294, Loss: 1.042601483709672\n",
      "Epoch 295, Loss: 1.0706182052107418\n",
      "Epoch 296, Loss: 1.08518585562706\n",
      "Epoch 297, Loss: 1.060579305185991\n",
      "Epoch 298, Loss: 1.0640900643432842\n",
      "Epoch 299, Loss: 1.068522036075592\n",
      "Epoch 300, Loss: 1.0607289882267223\n",
      "Epoch 301, Loss: 1.0577631347319658\n",
      "Epoch 302, Loss: 1.053358512766221\n",
      "Epoch 303, Loss: 1.0510757144759684\n",
      "Epoch 304, Loss: 1.059601045706693\n",
      "Epoch 305, Loss: 1.0736512734609491\n",
      "Epoch 306, Loss: 1.0396164568031536\n",
      "Epoch 307, Loss: 1.0630597051452189\n",
      "Epoch 308, Loss: 1.054682228495093\n",
      "Epoch 309, Loss: 1.0509093403816223\n",
      "Epoch 310, Loss: 1.0504417472025926\n",
      "Epoch 311, Loss: 1.0572239563745611\n",
      "Epoch 312, Loss: 1.053272948545568\n",
      "Epoch 313, Loss: 1.0511578356518465\n",
      "Epoch 314, Loss: 1.0729416451033424\n",
      "Epoch 315, Loss: 1.0740294999936049\n",
      "Epoch 316, Loss: 1.0438359660260819\n",
      "Epoch 317, Loss: 1.0547104968744165\n",
      "Epoch 318, Loss: 1.0648793350247776\n",
      "Epoch 319, Loss: 1.0413666367530823\n",
      "Epoch 320, Loss: 1.0614188927061416\n",
      "Epoch 321, Loss: 1.0526631754987381\n",
      "Epoch 322, Loss: 1.0629787374945248\n",
      "Epoch 323, Loss: 1.0635063683285433\n",
      "Epoch 324, Loss: 1.0480233518516315\n",
      "Epoch 325, Loss: 1.051813090548796\n",
      "Epoch 326, Loss: 1.0448682588689469\n",
      "Epoch 327, Loss: 1.0424026163185345\n",
      "Epoch 328, Loss: 1.0403243724037619\n",
      "Epoch 329, Loss: 1.0367005975807415\n",
      "Epoch 330, Loss: 1.075476248474682\n",
      "Epoch 331, Loss: 1.0462902591508978\n",
      "Epoch 332, Loss: 1.0586435847422655\n",
      "Epoch 333, Loss: 1.0695603279506458\n",
      "Epoch 334, Loss: 1.0588348683188944\n",
      "Epoch 335, Loss: 1.0538646754096537\n",
      "Epoch 336, Loss: 1.0520384311676025\n",
      "Epoch 337, Loss: 1.0591785241575802\n",
      "Epoch 338, Loss: 1.0381681831444012\n",
      "Epoch 339, Loss: 1.0538554226650911\n",
      "Epoch 340, Loss: 1.0479901362867916\n",
      "Epoch 341, Loss: 1.0467134170672472\n",
      "Epoch 342, Loss: 1.0483833972145529\n",
      "Epoch 343, Loss: 1.0487853043219622\n",
      "Epoch 344, Loss: 1.0488347639055813\n",
      "Epoch 345, Loss: 1.0371833233272327\n",
      "Epoch 346, Loss: 1.0556174201123856\n",
      "Epoch 347, Loss: 1.0498535422717823\n",
      "Epoch 348, Loss: 1.032989766667871\n",
      "Epoch 349, Loss: 1.0440509669920977\n",
      "Epoch 350, Loss: 1.040193398209179\n",
      "Epoch 351, Loss: 1.0376701565349804\n",
      "Epoch 352, Loss: 1.0589672029018402\n",
      "Epoch 353, Loss: 1.058032528442495\n",
      "Epoch 354, Loss: 1.0584533898269428\n",
      "Epoch 355, Loss: 1.0364746065700756\n",
      "Epoch 356, Loss: 1.0539474978166468\n",
      "Epoch 357, Loss: 1.061178759616964\n",
      "Epoch 358, Loss: 1.039459079504013\n",
      "Epoch 359, Loss: 1.0402026106329525\n",
      "Epoch 360, Loss: 1.0110439027056974\n",
      "Epoch 361, Loss: 1.0717680401661818\n",
      "Epoch 362, Loss: 1.0349684392704683\n",
      "Epoch 363, Loss: 1.0413529487217175\n",
      "Epoch 364, Loss: 1.0477754421093886\n",
      "Epoch 365, Loss: 1.033473765148836\n",
      "Epoch 366, Loss: 1.047068550306208\n",
      "Epoch 367, Loss: 1.012249767780304\n",
      "Epoch 368, Loss: 1.0432546296540428\n",
      "Epoch 369, Loss: 1.037091817925958\n",
      "Epoch 370, Loss: 1.039513852666406\n",
      "Epoch 371, Loss: 1.049572245163076\n",
      "Epoch 372, Loss: 1.0418501671622782\n",
      "Epoch 373, Loss: 1.050602467621074\n",
      "Epoch 374, Loss: 1.0535135462003595\n",
      "Epoch 375, Loss: 1.0649515916319454\n",
      "Epoch 376, Loss: 1.0487449116566603\n",
      "Epoch 377, Loss: 1.064671888070948\n",
      "Epoch 378, Loss: 1.0397360763129067\n",
      "Epoch 379, Loss: 1.0454919285633986\n",
      "Epoch 380, Loss: 1.0417324900627136\n",
      "Epoch 381, Loss: 1.0602716172442717\n",
      "Epoch 382, Loss: 1.0542780897196602\n",
      "Epoch 383, Loss: 1.0407724064939163\n",
      "Epoch 384, Loss: 1.062476230018279\n",
      "Epoch 385, Loss: 1.055394270840813\n",
      "Epoch 386, Loss: 1.0540040854145498\n",
      "Epoch 387, Loss: 1.0680302539292503\n",
      "Epoch 388, Loss: 1.0683267344446743\n",
      "Epoch 389, Loss: 1.0326943397521973\n",
      "Epoch 390, Loss: 1.0468816108563368\n",
      "Epoch 391, Loss: 1.038872273529277\n",
      "Epoch 392, Loss: 1.0464425718083101\n",
      "Epoch 393, Loss: 1.0503671011504006\n",
      "Epoch 394, Loss: 1.0382637819823097\n",
      "Epoch 395, Loss: 1.0502834355129915\n",
      "Epoch 396, Loss: 1.0377219932920791\n",
      "Epoch 397, Loss: 1.0642072067541235\n",
      "Epoch 398, Loss: 1.054452063406215\n",
      "Epoch 399, Loss: 1.0212423573522007\n",
      "Epoch 400, Loss: 1.0368376251529245\n",
      "Epoch 401, Loss: 1.0497818028225618\n",
      "Epoch 402, Loss: 1.0469567933503319\n",
      "Epoch 403, Loss: 1.026783056118909\n",
      "Epoch 404, Loss: 1.0448112330015968\n",
      "Epoch 405, Loss: 1.0376960985800798\n",
      "Epoch 406, Loss: 1.0406624692327835\n",
      "Epoch 407, Loss: 1.047832471482894\n",
      "Epoch 408, Loss: 1.0374806400607615\n",
      "Epoch 409, Loss: 1.06225024952608\n",
      "Epoch 410, Loss: 1.0510160221773035\n",
      "Epoch 411, Loss: 1.045534339021234\n",
      "Epoch 412, Loss: 1.0385243752423454\n",
      "Epoch 413, Loss: 1.0350376665592194\n",
      "Epoch 414, Loss: 1.056983067708857\n",
      "Epoch 415, Loss: 1.0554942506201126\n",
      "Epoch 416, Loss: 1.04815015372108\n",
      "Epoch 417, Loss: 1.037375164382598\n",
      "Epoch 418, Loss: 1.0557141935124117\n",
      "Epoch 419, Loss: 1.0734364618273342\n",
      "Epoch 420, Loss: 1.0383627852972817\n",
      "Epoch 421, Loss: 1.057139259927413\n",
      "Epoch 422, Loss: 1.0514486975529616\n",
      "Epoch 423, Loss: 1.0494998220135183\n",
      "Epoch 424, Loss: 1.0668937669080847\n",
      "Epoch 425, Loss: 1.040585861486547\n",
      "Epoch 426, Loss: 1.0481898118467892\n",
      "Epoch 427, Loss: 1.0520755508366753\n",
      "Epoch 428, Loss: 1.0483969239627613\n",
      "Epoch 429, Loss: 1.0450093290385079\n",
      "Epoch 430, Loss: 1.037582227412392\n",
      "Epoch 431, Loss: 1.0423957004266626\n",
      "Epoch 432, Loss: 1.057684582822463\n",
      "Epoch 433, Loss: 1.0357750110766466\n",
      "Epoch 434, Loss: 1.077686434282976\n",
      "Epoch 435, Loss: 1.0435232449980343\n",
      "Epoch 436, Loss: 1.0589448125923382\n",
      "Epoch 437, Loss: 1.0523986132705914\n",
      "Epoch 438, Loss: 1.03539657943389\n",
      "Epoch 439, Loss: 1.0611152771641226\n",
      "Epoch 440, Loss: 1.0393217942293953\n",
      "Epoch 441, Loss: 1.0668166279792786\n",
      "Epoch 442, Loss: 1.0513167749432957\n",
      "Epoch 443, Loss: 1.047674866283641\n",
      "Epoch 444, Loss: 1.0379702816991245\n",
      "Epoch 445, Loss: 1.0522264294764574\n",
      "Epoch 446, Loss: 1.0540911765659557\n",
      "Epoch 447, Loss: 1.043191927320817\n",
      "Epoch 448, Loss: 1.0516180992126465\n",
      "Epoch 449, Loss: 1.0514232060488533\n",
      "Epoch 450, Loss: 1.029407509986092\n",
      "Epoch 451, Loss: 1.0301051455385544\n",
      "Epoch 452, Loss: 1.0390814858324386\n",
      "Epoch 453, Loss: 1.0384553092367508\n",
      "Epoch 454, Loss: 1.040909798706279\n",
      "Epoch 455, Loss: 1.046129221425337\n",
      "Epoch 456, Loss: 1.0305837480460895\n",
      "Epoch 457, Loss: 1.0695571583860062\n",
      "Epoch 458, Loss: 1.0509580075740814\n",
      "Epoch 459, Loss: 1.0630558220779194\n",
      "Epoch 460, Loss: 1.063738027039696\n",
      "Epoch 461, Loss: 1.0672682032865637\n",
      "Epoch 462, Loss: 1.0569589085438673\n",
      "Epoch 463, Loss: 1.048980399089701\n",
      "Epoch 464, Loss: 1.0573661292300505\n",
      "Epoch 465, Loss: 1.0390340843621422\n",
      "Epoch 466, Loss: 1.0517123586991255\n",
      "Epoch 467, Loss: 1.0701253659584944\n",
      "Epoch 468, Loss: 1.0311173323322744\n",
      "Epoch 469, Loss: 1.0487342634621788\n",
      "Epoch 470, Loss: 1.0299786294207853\n",
      "Epoch 471, Loss: 1.0414181176353903\n",
      "Epoch 472, Loss: 1.0495302379131317\n",
      "Epoch 473, Loss: 1.0392929473344017\n",
      "Epoch 474, Loss: 1.0636955289279713\n",
      "Epoch 475, Loss: 1.043897386859445\n",
      "Epoch 476, Loss: 1.0718324184417725\n",
      "Epoch 477, Loss: 1.0495592320666594\n",
      "Epoch 478, Loss: 1.0320316973854513\n",
      "Epoch 479, Loss: 1.0445493442170761\n",
      "Epoch 480, Loss: 1.0353564357056337\n",
      "Epoch 481, Loss: 1.0465830284006454\n",
      "Epoch 482, Loss: 1.0377092712065752\n",
      "Epoch 483, Loss: 1.0534962433225967\n",
      "Epoch 484, Loss: 1.0456781422390657\n",
      "Epoch 485, Loss: 1.0507470176500433\n",
      "Epoch 486, Loss: 1.044871244360419\n",
      "Epoch 487, Loss: 1.028958813232534\n",
      "Epoch 488, Loss: 1.0641629029722774\n",
      "Epoch 489, Loss: 1.0391359785023857\n",
      "Epoch 490, Loss: 1.0487965906367582\n",
      "Epoch 491, Loss: 1.0550883272114921\n",
      "Epoch 492, Loss: 1.041312023120768\n",
      "Epoch 493, Loss: 1.0554622990243576\n",
      "Epoch 494, Loss: 1.0347749590873718\n",
      "Epoch 495, Loss: 1.0516073318088757\n",
      "Epoch 496, Loss: 1.0360345402184654\n",
      "Epoch 497, Loss: 1.0716179433990927\n",
      "Epoch 498, Loss: 1.050000218784108\n",
      "Epoch 499, Loss: 1.0466116631732267\n",
      "Epoch 500, Loss: 1.0360918237882502\n",
      "Epoch 501, Loss: 1.056939941995284\n",
      "Epoch 502, Loss: 1.0579061420524822\n",
      "Epoch 503, Loss: 1.04377182616907\n",
      "Epoch 504, Loss: 1.0624875128269196\n",
      "Epoch 505, Loss: 1.0510424340472502\n",
      "Epoch 506, Loss: 1.059667036813848\n",
      "Epoch 507, Loss: 1.0250470883706038\n",
      "Epoch 508, Loss: 1.0395031098057241\n",
      "Epoch 509, Loss: 1.0555239179555107\n",
      "Epoch 510, Loss: 1.0474977055016685\n",
      "Epoch 511, Loss: 1.0497738087878508\n",
      "Epoch 512, Loss: 1.0218120094607859\n",
      "Epoch 513, Loss: 1.0667480388108421\n",
      "Epoch 514, Loss: 1.0286605305531447\n",
      "Epoch 515, Loss: 1.07318194473491\n",
      "Epoch 516, Loss: 1.0356997321633732\n",
      "Epoch 517, Loss: 1.0554113686084747\n",
      "Epoch 518, Loss: 1.030903763630811\n",
      "Epoch 519, Loss: 1.0578122033792383\n",
      "Epoch 520, Loss: 1.042580246925354\n",
      "Epoch 521, Loss: 1.0501255375497482\n",
      "Epoch 522, Loss: 1.056908740716822\n",
      "Epoch 523, Loss: 1.0392907700117897\n",
      "Epoch 524, Loss: 1.0466533916838028\n",
      "Epoch 525, Loss: 1.0317506422014797\n",
      "Epoch 526, Loss: 1.021931206478792\n",
      "Epoch 527, Loss: 1.0340327830875622\n",
      "Epoch 528, Loss: 1.0389565439785229\n",
      "Epoch 529, Loss: 1.0462680634330301\n",
      "Epoch 530, Loss: 1.0485902814304127\n",
      "Epoch 531, Loss: 1.0561280986841988\n",
      "Epoch 532, Loss: 1.0360719947253956\n",
      "Epoch 533, Loss: 1.0504710656755112\n",
      "Epoch 534, Loss: 1.0422318016781527\n",
      "Epoch 535, Loss: 1.0425017686451183\n",
      "Epoch 536, Loss: 1.0363934899077696\n",
      "Epoch 537, Loss: 1.0461482475785648\n",
      "Epoch 538, Loss: 1.0511233280686771\n",
      "Epoch 539, Loss: 1.029397899613661\n",
      "Epoch 540, Loss: 1.0398065237437977\n",
      "Epoch 541, Loss: 1.0492331350550932\n",
      "Epoch 542, Loss: 1.0597160125479979\n",
      "Epoch 543, Loss: 1.0374406015171724\n",
      "Epoch 544, Loss: 1.0474594550974228\n",
      "Epoch 545, Loss: 1.0326245953054989\n",
      "Epoch 546, Loss: 1.034454897922628\n",
      "Epoch 547, Loss: 1.0518416499390322\n",
      "Epoch 548, Loss: 1.0361267310731552\n",
      "Epoch 549, Loss: 1.0283584594726562\n",
      "Epoch 550, Loss: 1.0260428505785324\n",
      "Epoch 551, Loss: 1.0380985088208143\n",
      "Epoch 552, Loss: 1.033134961829466\n",
      "Epoch 553, Loss: 1.051365284358754\n",
      "Epoch 554, Loss: 1.030995572314543\n",
      "Epoch 555, Loss: 1.0549523970660042\n",
      "Epoch 556, Loss: 1.049514295423732\n",
      "Epoch 557, Loss: 1.0572930837378782\n",
      "Epoch 558, Loss: 1.0328915574971367\n",
      "Epoch 559, Loss: 1.0486722343108232\n",
      "Epoch 560, Loss: 1.0224555082180922\n",
      "Epoch 561, Loss: 1.0465204838444204\n",
      "Epoch 562, Loss: 1.0284718415316414\n",
      "Epoch 563, Loss: 1.0083124497357536\n",
      "Epoch 564, Loss: 1.0110281022155987\n",
      "Epoch 565, Loss: 1.0451292237814735\n",
      "Epoch 566, Loss: 1.0278492938069737\n",
      "Epoch 567, Loss: 1.0396978522048277\n",
      "Epoch 568, Loss: 1.0462361696888418\n",
      "Epoch 569, Loss: 1.032054503174389\n",
      "Epoch 570, Loss: 1.071882072617026\n",
      "Epoch 571, Loss: 1.04122595576679\n",
      "Epoch 572, Loss: 1.0135440247900345\n",
      "Epoch 573, Loss: 1.040791294153999\n",
      "Epoch 574, Loss: 1.016576400574516\n",
      "Epoch 575, Loss: 1.0130397291744457\n",
      "Epoch 576, Loss: 1.042308344560511\n",
      "Epoch 577, Loss: 1.065493362791398\n",
      "Epoch 578, Loss: 1.0499668016153223\n",
      "Epoch 579, Loss: 1.0374884833307827\n",
      "Epoch 580, Loss: 1.0244075480629415\n",
      "Epoch 581, Loss: 1.044824125135646\n",
      "Epoch 582, Loss: 1.0158880335443161\n",
      "Epoch 583, Loss: 1.0240337217555326\n",
      "Epoch 584, Loss: 1.042466228499132\n",
      "Epoch 585, Loss: 1.0425415249431835\n",
      "Epoch 586, Loss: 1.021454293938244\n",
      "Epoch 587, Loss: 1.0516575259320877\n",
      "Epoch 588, Loss: 1.0558137280099533\n",
      "Epoch 589, Loss: 1.041798200677423\n",
      "Epoch 590, Loss: 1.052999561323839\n",
      "Epoch 591, Loss: 1.0249182318939882\n",
      "Epoch 592, Loss: 1.0411299642394571\n",
      "Epoch 593, Loss: 1.0405533857205336\n",
      "Epoch 594, Loss: 1.0101019775166231\n",
      "Epoch 595, Loss: 1.0345604910570032\n",
      "Epoch 596, Loss: 1.0273293835275314\n",
      "Epoch 597, Loss: 1.0271075438050663\n",
      "Epoch 598, Loss: 1.049317659700618\n",
      "Epoch 599, Loss: 1.014941229539759\n",
      "Epoch 600, Loss: 1.0516342099975138\n",
      "Epoch 601, Loss: 1.0385897843276752\n",
      "Epoch 602, Loss: 1.0144357856582193\n",
      "Epoch 603, Loss: 1.0734565959257238\n",
      "Epoch 604, Loss: 1.007146856364082\n",
      "Epoch 605, Loss: 1.0555123388767242\n",
      "Epoch 606, Loss: 1.0074724379707785\n",
      "Epoch 607, Loss: 1.0518498613553888\n",
      "Epoch 608, Loss: 1.0350330068784601\n",
      "Epoch 609, Loss: 1.0471042447230394\n",
      "Epoch 610, Loss: 1.0151482943226309\n",
      "Epoch 611, Loss: 1.0379222000346464\n",
      "Epoch 612, Loss: 1.056855056215735\n",
      "Epoch 613, Loss: 1.0574542985242956\n",
      "Epoch 614, Loss: 1.0336486311519848\n",
      "Epoch 615, Loss: 1.0402715907377356\n",
      "Epoch 616, Loss: 1.044744007727679\n",
      "Epoch 617, Loss: 1.0548444811035604\n",
      "Epoch 618, Loss: 1.0614278036005356\n",
      "Epoch 619, Loss: 1.048678357811535\n",
      "Epoch 620, Loss: 1.0306425795835608\n",
      "Epoch 621, Loss: 1.017491324859507\n",
      "Epoch 622, Loss: 1.0384681189761442\n",
      "Epoch 623, Loss: 1.0506307833334978\n",
      "Epoch 624, Loss: 1.0571005852783428\n",
      "Epoch 625, Loss: 1.055877082488116\n",
      "Epoch 626, Loss: 1.0630311317303602\n",
      "Epoch 627, Loss: 1.045204709557926\n",
      "Epoch 628, Loss: 1.0526583457694334\n",
      "Epoch 629, Loss: 1.0404000825741713\n",
      "Epoch 630, Loss: 1.0256598819704617\n",
      "Epoch 631, Loss: 1.0562312795835382\n",
      "Epoch 632, Loss: 1.0143777693019194\n",
      "Epoch 633, Loss: 1.0383937341325424\n",
      "Epoch 634, Loss: 1.0336210043991314\n",
      "Epoch 635, Loss: 1.0279213786125183\n",
      "Epoch 636, Loss: 1.0550059693701126\n",
      "Epoch 637, Loss: 1.039415345472448\n",
      "Epoch 638, Loss: 1.0467774622580583\n",
      "Epoch 639, Loss: 1.025546685737722\n",
      "Epoch 640, Loss: 1.0279136931194979\n",
      "Epoch 641, Loss: 1.041369979872423\n",
      "Epoch 642, Loss: 1.0356953845304602\n",
      "Epoch 643, Loss: 1.0293853475767023\n",
      "Epoch 644, Loss: 1.0392792522907257\n",
      "Epoch 645, Loss: 1.0292927868225996\n",
      "Epoch 646, Loss: 1.0293790294843561\n",
      "Epoch 647, Loss: 1.067760739256354\n",
      "Epoch 648, Loss: 1.0600731600733364\n",
      "Epoch 649, Loss: 1.0538349905434776\n",
      "Epoch 650, Loss: 1.017431429203819\n",
      "Epoch 651, Loss: 1.0345415069776422\n",
      "Epoch 652, Loss: 1.0530733010348152\n",
      "Epoch 653, Loss: 1.029017628992305\n",
      "Epoch 654, Loss: 1.0398959833032944\n",
      "Epoch 655, Loss: 1.0433864681159748\n",
      "Epoch 656, Loss: 1.0445294415249544\n",
      "Epoch 657, Loss: 1.0377043853787815\n",
      "Epoch 658, Loss: 1.0479560722323025\n",
      "Epoch 659, Loss: 1.0413755704374874\n",
      "Epoch 660, Loss: 1.055714891237371\n",
      "Epoch 661, Loss: 1.0382229896152722\n",
      "Epoch 662, Loss: 1.0426749446812797\n",
      "Epoch 663, Loss: 1.0460572242736816\n",
      "Epoch 664, Loss: 1.0270972953123205\n",
      "Epoch 665, Loss: 1.0542730952010435\n",
      "Epoch 666, Loss: 1.0361183303243973\n",
      "Epoch 667, Loss: 1.0484613565837635\n",
      "Epoch 668, Loss: 1.041060496779049\n",
      "Epoch 669, Loss: 1.0286442532258875\n",
      "Epoch 670, Loss: 1.051932825761683\n",
      "Epoch 671, Loss: 1.0399940715116613\n",
      "Epoch 672, Loss: 1.0565107219359453\n",
      "Epoch 673, Loss: 1.049339753739974\n",
      "Epoch 674, Loss: 1.0583474846447216\n",
      "Epoch 675, Loss: 1.0344755526851206\n",
      "Epoch 676, Loss: 1.0135682908927692\n",
      "Epoch 677, Loss: 1.0170882922761582\n",
      "Epoch 678, Loss: 1.062326936160817\n",
      "Epoch 679, Loss: 1.0477945436449612\n",
      "Epoch 680, Loss: 1.0420019100694096\n",
      "Epoch 681, Loss: 1.038526887402815\n",
      "Epoch 682, Loss: 1.0709040445439957\n",
      "Epoch 683, Loss: 1.0553974789731644\n",
      "Epoch 684, Loss: 1.0503786521799423\n",
      "Epoch 685, Loss: 1.0296396315097809\n",
      "Epoch 686, Loss: 1.0392569198327906\n",
      "Epoch 687, Loss: 1.0344229813884287\n",
      "Epoch 688, Loss: 1.0304878599503462\n",
      "Epoch 689, Loss: 1.0544647381586187\n",
      "Epoch 690, Loss: 1.0325766994672663\n",
      "Epoch 691, Loss: 1.0206263871753918\n",
      "Epoch 692, Loss: 1.0313524709028357\n",
      "Epoch 693, Loss: 1.0105826819644255\n",
      "Epoch 694, Loss: 1.0649215912117678\n",
      "Epoch 695, Loss: 1.0522712171077728\n",
      "Epoch 696, Loss: 1.0212321439210106\n",
      "Epoch 697, Loss: 1.0314518037964315\n",
      "Epoch 698, Loss: 1.0545712288688212\n",
      "Epoch 699, Loss: 1.027714284027324\n",
      "Epoch 700, Loss: 1.0264243238112505\n",
      "Epoch 701, Loss: 1.0600293576717377\n",
      "Epoch 702, Loss: 1.0571226828238542\n",
      "Epoch 703, Loss: 1.043760795803631\n",
      "Epoch 704, Loss: 1.0457857093390297\n",
      "Epoch 705, Loss: 1.039380785296945\n",
      "Epoch 706, Loss: 1.0307803294237923\n",
      "Epoch 707, Loss: 1.0289609572466683\n",
      "Epoch 708, Loss: 1.0388002605999218\n",
      "Epoch 709, Loss: 1.04640151998576\n",
      "Epoch 710, Loss: 1.034306962700451\n",
      "Epoch 711, Loss: 1.0257625737610985\n",
      "Epoch 712, Loss: 1.0112659808467417\n",
      "Epoch 713, Loss: 1.0448031705968521\n",
      "Epoch 714, Loss: 1.031642694683636\n",
      "Epoch 715, Loss: 1.0284553292919607\n",
      "Epoch 716, Loss: 1.0300762951374054\n",
      "Epoch 717, Loss: 1.0490856503739077\n",
      "Epoch 718, Loss: 1.0505084377877854\n",
      "Epoch 719, Loss: 1.0324546782409443\n",
      "Epoch 720, Loss: 1.0581507560084848\n",
      "Epoch 721, Loss: 1.042639541275361\n",
      "Epoch 722, Loss: 1.0291221527492298\n",
      "Epoch 723, Loss: 1.0625677704811096\n",
      "Epoch 724, Loss: 1.0364165183375864\n",
      "Epoch 725, Loss: 1.0418324996443356\n",
      "Epoch 726, Loss: 1.0184401378912085\n",
      "Epoch 727, Loss: 1.0397380835869734\n",
      "Epoch 728, Loss: 1.0254682232351864\n",
      "Epoch 729, Loss: 1.0304916904253119\n",
      "Epoch 730, Loss: 1.0473502085489386\n",
      "Epoch 731, Loss: 1.0069053646396189\n",
      "Epoch 732, Loss: 1.0454655377303852\n",
      "Epoch 733, Loss: 1.035474519519245\n",
      "Epoch 734, Loss: 1.037552081486758\n",
      "Epoch 735, Loss: 1.0501597085419823\n",
      "Epoch 736, Loss: 1.0343681100536795\n",
      "Epoch 737, Loss: 1.016574740409851\n",
      "Epoch 738, Loss: 1.0155799090862274\n",
      "Epoch 739, Loss: 1.028146174024133\n",
      "Epoch 740, Loss: 1.011616794502034\n",
      "Epoch 741, Loss: 1.0383775935453528\n",
      "Epoch 742, Loss: 1.0460516302024616\n",
      "Epoch 743, Loss: 1.028345491956262\n",
      "Epoch 744, Loss: 1.0478857317391563\n",
      "Epoch 745, Loss: 1.068698309800204\n",
      "Epoch 746, Loss: 1.0349525756695692\n",
      "Epoch 747, Loss: 1.0216747382107902\n",
      "Epoch 748, Loss: 1.03902471240829\n",
      "Epoch 749, Loss: 1.0362218706046833\n",
      "Epoch 750, Loss: 1.0379022061824799\n",
      "Epoch 751, Loss: 1.044758446076337\n",
      "Epoch 752, Loss: 1.0351724186364342\n",
      "Epoch 753, Loss: 1.0309806395979488\n",
      "Epoch 754, Loss: 1.0290715887266046\n",
      "Epoch 755, Loss: 1.0311037337078768\n",
      "Epoch 756, Loss: 1.028094263637767\n",
      "Epoch 757, Loss: 1.0330085701802199\n",
      "Epoch 758, Loss: 1.0448389298775618\n",
      "Epoch 759, Loss: 1.0072192654890173\n",
      "Epoch 760, Loss: 1.0386689918882706\n",
      "Epoch 761, Loss: 1.0147287074257345\n",
      "Epoch 762, Loss: 1.0153685843243319\n",
      "Epoch 763, Loss: 1.0261747416327982\n",
      "Epoch 764, Loss: 1.0136505172533148\n",
      "Epoch 765, Loss: 1.0470033919110018\n",
      "Epoch 766, Loss: 1.038129161385929\n",
      "Epoch 767, Loss: 1.0492955341058618\n",
      "Epoch 768, Loss: 1.0303065163247727\n",
      "Epoch 769, Loss: 1.003635567777297\n",
      "Epoch 770, Loss: 1.048841365996529\n",
      "Epoch 771, Loss: 1.0370768063208635\n",
      "Epoch 772, Loss: 1.01483613077332\n",
      "Epoch 773, Loss: 1.036497233545079\n",
      "Epoch 774, Loss: 1.02833008240251\n",
      "Epoch 775, Loss: 1.0303384521428276\n",
      "Epoch 776, Loss: 1.0323572158813477\n",
      "Epoch 777, Loss: 1.0436854590387905\n",
      "Epoch 778, Loss: 1.0278862258967232\n",
      "Epoch 779, Loss: 1.0553541902233572\n",
      "Epoch 780, Loss: 1.0443605517639833\n",
      "Epoch 781, Loss: 1.0310717172482435\n",
      "Epoch 782, Loss: 1.0475133289309109\n",
      "Epoch 783, Loss: 1.0163383343640495\n",
      "Epoch 784, Loss: 1.013723008772906\n",
      "Epoch 785, Loss: 1.0348518946591545\n",
      "Epoch 786, Loss: 1.0202937161221224\n",
      "Epoch 787, Loss: 1.0438886179643518\n",
      "Epoch 788, Loss: 1.0567107726545895\n",
      "Epoch 789, Loss: 1.0280275344848633\n",
      "Epoch 790, Loss: 1.0257117274929495\n",
      "Epoch 791, Loss: 1.0292244518504423\n",
      "Epoch 792, Loss: 1.020059478633544\n",
      "Epoch 793, Loss: 1.0275016690001768\n",
      "Epoch 794, Loss: 1.0262861199238722\n",
      "Epoch 795, Loss: 1.008474935503567\n",
      "Epoch 796, Loss: 1.0142386783571804\n",
      "Epoch 797, Loss: 1.0520777684800766\n",
      "Epoch 798, Loss: 1.0158900162752937\n",
      "Epoch 799, Loss: 1.034181510700899\n",
      "Epoch 800, Loss: 1.0321211885003483\n",
      "Epoch 801, Loss: 1.0266032727325665\n",
      "Epoch 802, Loss: 1.050922039677115\n",
      "Epoch 803, Loss: 1.0442693338674658\n",
      "Epoch 804, Loss: 1.0139617832267986\n",
      "Epoch 805, Loss: 1.0277151465415955\n",
      "Epoch 806, Loss: 1.027642804033616\n",
      "Epoch 807, Loss: 1.0183360909714418\n",
      "Epoch 808, Loss: 1.0132142673520481\n",
      "Epoch 809, Loss: 1.0201693282407873\n",
      "Epoch 810, Loss: 1.039450145819608\n",
      "Epoch 811, Loss: 1.043730055584627\n",
      "Epoch 812, Loss: 1.015883710454492\n",
      "Epoch 813, Loss: 1.01760934380924\n",
      "Epoch 814, Loss: 1.0179029457709368\n",
      "Epoch 815, Loss: 1.014514065840665\n",
      "Epoch 816, Loss: 1.063130985288059\n",
      "Epoch 817, Loss: 1.0341540680212133\n",
      "Epoch 818, Loss: 1.0421009905198042\n",
      "Epoch 819, Loss: 1.0446180683725022\n",
      "Epoch 820, Loss: 1.0054277391994701\n",
      "Epoch 821, Loss: 1.0480187728124506\n",
      "Epoch 822, Loss: 1.02424716072924\n",
      "Epoch 823, Loss: 1.0497007527772118\n",
      "Epoch 824, Loss: 1.02186238415101\n",
      "Epoch 825, Loss: 1.0289627523983227\n",
      "Epoch 826, Loss: 1.0288813832928152\n",
      "Epoch 827, Loss: 1.0348810991820168\n",
      "Epoch 828, Loss: 1.0474341161110823\n",
      "Epoch 829, Loss: 1.0181472020990707\n",
      "Epoch 830, Loss: 1.0219504780629103\n",
      "Epoch 831, Loss: 1.0226733912439907\n",
      "Epoch 832, Loss: 1.0349385615657358\n",
      "Epoch 833, Loss: 1.0516798934515785\n",
      "Epoch 834, Loss: 1.038603093694238\n",
      "Epoch 835, Loss: 1.032383548862794\n",
      "Epoch 836, Loss: 1.0320026576519012\n",
      "Epoch 837, Loss: 1.0164242860148935\n",
      "Epoch 838, Loss: 1.016422953675775\n",
      "Epoch 839, Loss: 1.0286836448837728\n",
      "Epoch 840, Loss: 1.0346643924713135\n",
      "Epoch 841, Loss: 1.0280789487502153\n",
      "Epoch 842, Loss: 1.0166366626234615\n",
      "Epoch 843, Loss: 1.0228432241608114\n",
      "Epoch 844, Loss: 1.0178508968914257\n",
      "Epoch 845, Loss: 1.042662955382291\n",
      "Epoch 846, Loss: 1.0214283431277555\n",
      "Epoch 847, Loss: 1.0033582063282238\n",
      "Epoch 848, Loss: 1.021654083448298\n",
      "Epoch 849, Loss: 1.0390864856102888\n",
      "Epoch 850, Loss: 1.0103461567093344\n",
      "Epoch 851, Loss: 1.023006235851961\n",
      "Epoch 852, Loss: 1.0154259871034061\n",
      "Epoch 853, Loss: 1.0227895571905024\n",
      "Epoch 854, Loss: 1.0132877458544338\n",
      "Epoch 855, Loss: 1.025284353424521\n",
      "Epoch 856, Loss: 1.0283680786104763\n",
      "Epoch 857, Loss: 1.0157953325439901\n",
      "Epoch 858, Loss: 1.0362015945069931\n",
      "Epoch 859, Loss: 1.0009544404113995\n",
      "Epoch 860, Loss: 1.034225765396567\n",
      "Epoch 861, Loss: 1.0003614039982067\n",
      "Epoch 862, Loss: 1.023912233464858\n",
      "Epoch 863, Loss: 1.0574050352853888\n",
      "Epoch 864, Loss: 1.031324986149283\n",
      "Epoch 865, Loss: 1.0291171792675466\n",
      "Epoch 866, Loss: 1.0202241382178139\n",
      "Epoch 867, Loss: 1.0118586228174322\n",
      "Epoch 868, Loss: 1.0250341962365543\n",
      "Epoch 869, Loss: 1.0151748341672562\n",
      "Epoch 870, Loss: 1.0068049746401169\n",
      "Epoch 871, Loss: 1.0392939956749188\n",
      "Epoch 872, Loss: 1.0257957735482384\n",
      "Epoch 873, Loss: 1.0520951782955843\n",
      "Epoch 874, Loss: 1.0222717909251942\n",
      "Epoch 875, Loss: 1.0574439553653492\n",
      "Epoch 876, Loss: 1.0131626812850727\n",
      "Epoch 877, Loss: 1.0224834663026474\n",
      "Epoch 878, Loss: 1.0470972955226898\n",
      "Epoch 879, Loss: 1.0274609152008505\n",
      "Epoch 880, Loss: 1.0350565121454351\n",
      "Epoch 881, Loss: 0.9969882842372445\n",
      "Epoch 882, Loss: 1.0347815454006195\n",
      "Epoch 883, Loss: 1.042441709953196\n",
      "Epoch 884, Loss: 1.0304148582851185\n",
      "Epoch 885, Loss: 1.0376677705961115\n",
      "Epoch 886, Loss: 1.0457966502975016\n",
      "Epoch 887, Loss: 1.0488160480471218\n",
      "Epoch 888, Loss: 1.029033850221073\n",
      "Epoch 889, Loss: 1.0063614249229431\n",
      "Epoch 890, Loss: 1.0323506102842444\n",
      "Epoch 891, Loss: 1.0319391015697927\n",
      "Epoch 892, Loss: 1.0378001928329468\n",
      "Epoch 893, Loss: 1.025105159072315\n",
      "Epoch 894, Loss: 1.0142693940330954\n",
      "Epoch 895, Loss: 1.0562981209334206\n",
      "Epoch 896, Loss: 1.0519358866354998\n",
      "Epoch 897, Loss: 1.0314952941501843\n",
      "Epoch 898, Loss: 1.0311516470768873\n",
      "Epoch 899, Loss: 1.0079393001163708\n",
      "Epoch 900, Loss: 1.032677799463272\n",
      "Epoch 901, Loss: 1.0463396030313827\n",
      "Epoch 902, Loss: 1.052920185467776\n",
      "Epoch 903, Loss: 1.0375380060252022\n",
      "Epoch 904, Loss: 1.0210364636252909\n",
      "Epoch 905, Loss: 1.050262466949575\n",
      "Epoch 906, Loss: 1.0033950104432947\n",
      "Epoch 907, Loss: 1.0366511555279003\n",
      "Epoch 908, Loss: 1.0155011923874127\n",
      "Epoch 909, Loss: 1.0202266486252056\n",
      "Epoch 910, Loss: 1.0203132909886978\n",
      "Epoch 911, Loss: 1.0366350552614998\n",
      "Epoch 912, Loss: 1.0305719866472132\n",
      "Epoch 913, Loss: 1.0338207553414738\n",
      "Epoch 914, Loss: 0.9952850446981543\n",
      "Epoch 915, Loss: 1.0416843733366798\n",
      "Epoch 916, Loss: 1.0667056213406956\n",
      "Epoch 917, Loss: 1.0577705537571627\n",
      "Epoch 918, Loss: 1.0190104666878195\n",
      "Epoch 919, Loss: 1.0558663694297565\n",
      "Epoch 920, Loss: 1.0352449048967922\n",
      "Epoch 921, Loss: 1.0250959203523748\n",
      "Epoch 922, Loss: 1.0279943417100346\n",
      "Epoch 923, Loss: 1.0203708427793838\n",
      "Epoch 924, Loss: 1.0522974291268516\n",
      "Epoch 925, Loss: 1.0184994673027712\n",
      "Epoch 926, Loss: 1.0184005341109108\n",
      "Epoch 927, Loss: 1.0138905661947586\n",
      "Epoch 928, Loss: 1.0187647430335773\n",
      "Epoch 929, Loss: 1.0555082769954907\n",
      "Epoch 930, Loss: 1.0126425220685846\n",
      "Epoch 931, Loss: 1.0154531335129457\n",
      "Epoch 932, Loss: 1.0374358611948349\n",
      "Epoch 933, Loss: 1.0318650392925037\n",
      "Epoch 934, Loss: 1.0148514859816606\n",
      "Epoch 935, Loss: 1.0393684401231653\n",
      "Epoch 936, Loss: 1.015007197856903\n",
      "Epoch 937, Loss: 1.0047984193353092\n",
      "Epoch 938, Loss: 1.0457601617364323\n",
      "Epoch 939, Loss: 1.0219124310156877\n",
      "Epoch 940, Loss: 1.0183226588894339\n",
      "Epoch 941, Loss: 1.0280312597751617\n",
      "Epoch 942, Loss: 1.045036808532827\n",
      "Epoch 943, Loss: 1.0268986961420845\n",
      "Epoch 944, Loss: 1.0421529622638928\n",
      "Epoch 945, Loss: 1.0124309308388655\n",
      "Epoch 946, Loss: 1.024078269215191\n",
      "Epoch 947, Loss: 1.028920622432933\n",
      "Epoch 948, Loss: 1.0272540257257574\n",
      "Epoch 949, Loss: 1.029416874927633\n",
      "Epoch 950, Loss: 1.0688922142281252\n",
      "Epoch 951, Loss: 1.0220692140214584\n",
      "Epoch 952, Loss: 1.0246635535184074\n",
      "Epoch 953, Loss: 1.026300409260918\n",
      "Epoch 954, Loss: 1.0149792117231033\n",
      "Epoch 955, Loss: 1.031641143209794\n",
      "Epoch 956, Loss: 1.0039132482865278\n",
      "Epoch 957, Loss: 1.0471144914627075\n",
      "Epoch 958, Loss: 1.0370339733712814\n",
      "Epoch 959, Loss: 1.0115625297321993\n",
      "Epoch 960, Loss: 1.0365017775227041\n",
      "Epoch 961, Loss: 1.0452146021758808\n",
      "Epoch 962, Loss: 1.016920705051983\n",
      "Epoch 963, Loss: 1.023863681975533\n",
      "Epoch 964, Loss: 1.015440532389809\n",
      "Epoch 965, Loss: 1.0129700443323921\n",
      "Epoch 966, Loss: 1.0201836421209223\n",
      "Epoch 967, Loss: 1.024106371052125\n",
      "Epoch 968, Loss: 1.0325254657689262\n",
      "Epoch 969, Loss: 1.0294599760981167\n",
      "Epoch 970, Loss: 1.03510141898604\n",
      "Epoch 971, Loss: 1.0212228333248812\n",
      "Epoch 972, Loss: 1.0253088824889238\n",
      "Epoch 973, Loss: 1.0244588431190043\n",
      "Epoch 974, Loss: 1.038377260460573\n",
      "Epoch 975, Loss: 1.0451878326780655\n",
      "Epoch 976, Loss: 1.0487391440307392\n",
      "Epoch 977, Loss: 1.0220802268561195\n",
      "Epoch 978, Loss: 1.0213452700306387\n",
      "Epoch 979, Loss: 1.049227048369015\n",
      "Epoch 980, Loss: 1.0282795253921957\n",
      "Epoch 981, Loss: 1.0015797036535599\n",
      "Epoch 982, Loss: 1.0284364889649784\n",
      "Epoch 983, Loss: 1.0466638347681831\n",
      "Epoch 984, Loss: 1.0447298814268673\n",
      "Epoch 985, Loss: 1.0503642541520737\n",
      "Epoch 986, Loss: 1.020626394187703\n",
      "Epoch 987, Loss: 1.0355830788612366\n",
      "Epoch 988, Loss: 1.016941817367778\n",
      "Epoch 989, Loss: 1.0513559702564688\n",
      "Epoch 990, Loss: 1.0219747318941004\n",
      "Epoch 991, Loss: 1.0382547413601595\n",
      "Epoch 992, Loss: 1.0470194711404688\n",
      "Epoch 993, Loss: 1.003451273721807\n",
      "Epoch 994, Loss: 0.9924563222071704\n",
      "Epoch 995, Loss: 1.014539229519227\n",
      "Epoch 996, Loss: 1.0415273221100079\n",
      "Epoch 997, Loss: 1.0314219295978546\n",
      "Epoch 998, Loss: 1.000102814506082\n",
      "Epoch 999, Loss: 1.0175507296534145\n",
      "Epoch 1000, Loss: 1.0207246594569261\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test data: 35%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the test data: {100 * correct // total}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "#######################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
